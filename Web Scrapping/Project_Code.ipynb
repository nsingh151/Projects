{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries and this is a part of requirement file as well\n",
    "\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import http.cookiejar\n",
    "import time\n",
    "import http.cookiejar, urllib.request\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Category with search value\n",
    "\n",
    "['architects','interior-designers-and-decorators','civil-engineers-and-contractors' ]\n",
    "1. Architects & Building Design  - architects\n",
    "2. Interior Designers And Decorators - interior-designers-and-decorators\n",
    "3. Civil Engineers And Contractors - civil-engineers-and-contractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"header-2 header-dt-1 main-title\"\n",
    "cj = http.cookiejar.MozillaCookieJar()\n",
    "opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))\n",
    "\n",
    "category_list = ['architects','interior-designers-and-decorators','civil-engineers-and-contractors']\n",
    "page = \"https://www.houzz.in/professionals/\"+category_list[0]+\"/c/San-Francisco--CA\"\n",
    "print(\"Page URL : \", page)\n",
    "\n",
    "sourcecode = opener.open(page).read()\n",
    "soup = bs.BeautifulSoup(sourcecode)\n",
    "cj.clear_session_cookies()\n",
    "# print(soup.prettify())\n",
    "time.sleep(10)\n",
    "\n",
    "for titles in soup.findAll(\"h1\", class_=\"header-2 header-dt-1 main-title\"):\n",
    "    length_of_cat = titles.text.split()[0]\n",
    "    length_of_cat = length_of_cat.replace(\",\",\"\")\n",
    "    \n",
    "    print(length_of_cat)\n",
    "n_pages = int(length_of_cat) // 15\n",
    "print(n_pages)\n",
    "\n",
    "## For finding list of Contact urls on each category page \n",
    "link_list=[]\n",
    "for nav_pages in range(0, n_pages):\n",
    "    urls = \"https://www.houzz.in/professionals/\"+category_list[0]+\"/c/San-Francisco--CA/p/\"+str(nav_pages*15)\n",
    "    sourcecode = opener.open(urls).read()\n",
    "    soup_new = bs.BeautifulSoup(sourcecode)\n",
    "#     rq  = requests.get(urls)\n",
    "#     data = rq.text\n",
    "#     soup_new = bs.BeautifulSoup(data)\n",
    "    for links in soup_new.findAll(\"div\", class_=\"name-info\"):\n",
    "#         print(links)\n",
    "        for link in links.findAll(\"a\"):\n",
    "            test = link.get(\"href\")\n",
    "            if test ==\"javascript:;\" or 'browseReviews' in test.split(\"/\") :\n",
    "                continue\n",
    "            else:\n",
    "                print(\"test links\" , test)\n",
    "                link_list.append(link)\n",
    "    cj.clear_session_cookies()\n",
    "    time.sleep(10)\n",
    "\n",
    "    \n",
    "for i in link_list :\n",
    "    sourcecode = opener.open(i).read()\n",
    "    soup_text = bs.BeautifulSoup(sourcecode)\n",
    "    \n",
    "    contact_no = soup_text.findAll(\"span\", class_=\"pro-contact-text\").text\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "#         link = links.find(\"a\").get(\"href\")\n",
    "#         link_list.append(link)\n",
    "#     print(link_list)\n",
    "print(\"Length of Link List \",len(link_list))\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cj = http.cookiejar.MozillaCookieJar()\n",
    "opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))\n",
    "#r = opener.open(\"http://example.com/\")\n",
    "#cj.add_cookie_header({{'User-agent', 'Chrome/56.0.2924.87'}})\n",
    "# #opener.addheader = [{'User-agent', 'Chrome/56.0.2924.87'}]\n",
    "# cookies = http.cookiejar.CookieJar()\n",
    "\n",
    "\n",
    "class scarppy():\n",
    "    \n",
    "    def __init__ (self, url):\n",
    "        self.category_list = ['architects','interior-designers-and-decorators','civil-engineers-and-contractors' \n",
    "        self.url = url\n",
    "    \n",
    "    def category(self):\n",
    "        try:\n",
    "            page = \"https://www.houzz.in/professionals/\"+self.category_list[0]+\"/c/San-Francisco--CA\"\n",
    "            print(\"Page URL : \", page)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def fetch(self, url):\n",
    "        try:\n",
    "            \n",
    "            page = \"http://all-free-download.com/wallpapers/bridge\"\n",
    "            total_page = int(input(\"Enter the no. of pages to be get download: \"))\n",
    "\n",
    "            for i in range (1,total_page+1):\n",
    "\n",
    "                if i ==1:\n",
    "                    page = pag+\".html\"\n",
    "                else:\n",
    "                    page=pag+\"_page_\"+str(i)+\".html\"\n",
    "\n",
    "                sourcecode = opener.open(page).read()\n",
    "                soup = bs.BeautifulSoup(sourcecode,\"lxml\")\n",
    "                # print(soup)\n",
    "                for photos in soup.find_all(\"div\", class_=\"item\"):\n",
    "                        link = photos.find(\"a\").get(\"href\")\n",
    "                        self.download(link)\n",
    "                        # print(link)\n",
    "                cj.clear_session_cookies()\n",
    "        except Exception as e:\n",
    "            cj.clear_session_cookies()\n",
    "            print(\"Main Except Block\",str(e))\n",
    "\n",
    "    def download(self,link):\n",
    "        try:\n",
    "            source = opener.open(link).read()\n",
    "            sup = bs.BeautifulSoup(source,\"lxml\")\n",
    "            for downloads in sup.find_all(\"td\"):\n",
    "                download = downloads.find(\"a\").get(\"href\")\n",
    "                if '1280_720' in download:\n",
    "                    dwnload = download\n",
    "                    print(dwnload)\n",
    "                    self.selinum(download)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                # print(download)\n",
    "        except Exception as e:\n",
    "            cj.clear_session_cookies()\n",
    "            print(\"Download Except Block\",str(e))\n",
    "\n",
    "    def selinum(self,url):\n",
    "\n",
    "        try:\n",
    "            browser = webdriver.Chrome(executable_path=\"C:\\Python\\webdriver\")\n",
    "            browser.get(url)\n",
    "            time.sleep(10)\n",
    "            browser.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            cj.clear_session_cookies()\n",
    "            print(\"Selenium Except Block\",str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
